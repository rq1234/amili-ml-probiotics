{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from typing import List, Iterable, Callable, Tuple\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve, recall_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Callable, Union\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "SEED = 1729\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45156276",
   "metadata": {},
   "outputs": [],
   "source": [
    "abundance_full_df = pd.read_csv(r\"C:\\Users\\rongq\\OneDrive\\0 Rong\\01 Documents\\AMILI\\amili-ml-probiotics\\working_drafts\\datasets\\abundance.csv\")\n",
    "abundance_stool_full_df = pd.read_csv(r\"C:\\Users\\rongq\\OneDrive\\0 Rong\\01 Documents\\AMILI\\amili-ml-probiotics\\working_drafts\\datasets\\abundance_stoolsubset.csv\")\n",
    "abundance_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9916ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_list = abundance_full_df.loc[:,'disease'].unique()\n",
    "\n",
    "num_obs_list = [abundance_full_df.loc[abundance_full_df.disease == disease_list[i]\n",
    "                                      ].shape[0] for i in range(len(disease_list))]\n",
    "disease_obs_dict = {disease_list[i]:num_obs_list[i] for i in range(len(\n",
    "                                                            disease_list))}\n",
    "print(f\" Number of observations for each disease:\\n\" ,disease_obs_dict)\n",
    "low_data_diseases = [d for d in disease_list if disease_obs_dict[d] < 60]\n",
    "print('\\n diseases with less 60 samples:\\n',low_data_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40179c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge 'n', 'nd', 'leaness' into 'control'\n",
    "abundance_full_df.loc[:,'disease'] = abundance_full_df['disease'].apply(\n",
    "    lambda x: 'control' if ((x == 'n') or (x == 'nd') or (x == 'leaness')) else x)\n",
    "\n",
    "### merge 'ibd_crohn_disease' and 'ibd_ulcerative_colitis' into 'ibd'.\n",
    "abundance_full_df.loc[:,'disease'] = abundance_full_df['disease'].apply(\n",
    "    lambda x: 'ibd' if ('ibd' in x) else x)\n",
    "\n",
    "### merge 'small_adenoma' and 'large_adenoma' into 'adenoma'.\n",
    "abundance_full_df.loc[:,'disease'] = abundance_full_df['disease'].apply(\n",
    "    lambda x: 'adenoma' if ('adenoma' in x) else x)\n",
    "\n",
    "### retaining data only for the following diseases\n",
    "diseases = ['control', 'obesity', 'ibd', 'stec2-positive', \n",
    "            'impaired_glucose_tolerance', 'cirrhosis', 't2d', 'cancer', 'adenoma']\n",
    "abundance_df = abundance_full_df.loc[\n",
    "                            abundance_full_df['disease'].isin(diseases)]\n",
    "\n",
    "print(f'original dataframe shape is:  {abundance_full_df.shape}')\n",
    "print(f'selected dataframe shape is: {abundance_df.shape}')\n",
    "disease_list = abundance_df['disease'].unique()\n",
    "print(f\"total number of diseases is: {disease_list.size}\")\n",
    "\n",
    "num_obs_list = [abundance_df.loc[abundance_df.disease == disease_list[i]\n",
    "                                      ].shape[0] for i in range(len(disease_list))]\n",
    "disease_obs_dict = {disease_list[i]:num_obs_list[i] for i in range(len(\n",
    "                                                            disease_list))}\n",
    "print(f\"# of observations for each disease:\\n\",disease_obs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_index_dict = {d:diseases.index(d) for d in diseases}\n",
    "print(f'associating an id with each disease:')\n",
    "print(disease_index_dict)\n",
    "abundance_df['disease_id'] = abundance_df['disease'].apply(\n",
    "                                    lambda x: diseases.index(x))\n",
    "\n",
    "cols = abundance_df.columns.tolist()\n",
    "species = [x for x in cols if x.startswith('k_')]\n",
    "print(f'number of species is {len(species)}')\n",
    "metadata = [x for x in cols if not x.startswith('k_')]\n",
    "print(f'number of metadata columns is {len(metadata)}')\n",
    "\n",
    "species_df = abundance_df.loc[:,species].copy()\n",
    "species_df = species_df.astype('float32')\n",
    "abundance_df = pd.concat([abundance_df.loc[:,metadata], species_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_ids = np.arange(1, len(diseases)).tolist()\n",
    "test_disease_ids = random.sample(disease_ids, 2)\n",
    "print(f'test diseases:', [diseases[id] for id in test_disease_ids])\n",
    "\n",
    "remaining_disease_ids = [id for id in disease_ids if id not in test_disease_ids]\n",
    "valid_disease_ids = random.sample(remaining_disease_ids, 2)\n",
    "print(f'validation diseases:', [diseases[id] for id in valid_disease_ids])\n",
    "\n",
    "train_disease_ids = [diseases.index(d) \n",
    "          for d in diseases if diseases.index(d) not in test_disease_ids and \n",
    "                     diseases.index(d) not in valid_disease_ids]\n",
    "print(f'train diseases:', [diseases[id] for id in train_disease_ids])\n",
    "\n",
    "train_df = abundance_df.loc[abundance_df['disease_id'].isin(\n",
    "                                train_disease_ids)]\n",
    "print(train_df['disease_id'].unique())\n",
    "valid_df = abundance_df.loc[abundance_df['disease_id'].isin(\n",
    "                                valid_disease_ids)]\n",
    "print(valid_df['disease_id'].unique())\n",
    "test_df = abundance_df.loc[abundance_df['disease_id'].isin(\n",
    "                                test_disease_ids)]\n",
    "print(test_df['disease_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diseased_train_df = train_df.loc[train_df['disease_id'] != diseases.index('control')]\n",
    "print('train df has the following diseases:')\n",
    "print(diseased_train_df.disease.unique())\n",
    "counts, bins, histplot = plt.hist(diseased_train_df[species].std(), bins=20)\n",
    "select_species = np.array(species)[diseased_train_df[species].std().values>1.]\n",
    "print(f\"number of total species are: {len(species)}\")\n",
    "print(f\"number of selected species are: {select_species.size}\")\n",
    "\n",
    "select_species_id = np.append(select_species, ['disease_id'])\n",
    "train_df = train_df.loc[:,select_species_id]\n",
    "train_df['id'] = np.arange(train_df.shape[0])\n",
    "valid_df = valid_df.loc[:,select_species_id]\n",
    "valid_df['id'] = np.arange(valid_df.shape[0])\n",
    "test_df = test_df.loc[:,select_species_id]\n",
    "test_df['id'] = np.arange(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e46293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NShotTaskSampler(data.Sampler):\n",
    "    def __init__(self,\n",
    "                 dataset: torch.utils.data.Dataset,\n",
    "                 episodes_per_epoch: int = None,\n",
    "                 n: int = None,\n",
    "                 k: int = None,\n",
    "                 q: int = None,\n",
    "                 num_tasks: int = 1):\n",
    "        \"\"\"PyTorch Sampler subclass that generates batches of n-shot, k-way, \n",
    "        q-query tasks.\n",
    "\n",
    "        Each n-shot task contains a \"support set\" of `k` sets of `n` samples and \n",
    "        a \"query set\" of `k` sets of `q` samples. The support set and the query set \n",
    "        are all grouped into one Tensor such that the first n * k samples are from \n",
    "        the support set while the remaining q * k samples are from the query set.\n",
    "\n",
    "        The support and query sets are sampled such that they are disjoint \n",
    "        i.e. do not contain overlapping samples.\n",
    "\n",
    "        # Arguments\n",
    "            dataset: Instance of torch.utils.data.Dataset from which to draw samples\n",
    "            episodes_per_epoch: Arbitrary number of batches of n-shot tasks to \n",
    "                                generate in one epoch\n",
    "            n_shot: int. Number of samples for each class in the n-shot \n",
    "                            classification tasks.\n",
    "            k_way: int. Number of classes in the n-shot classification tasks.\n",
    "            q_queries: int. Number query samples for each class in the n-shot \n",
    "                            classification tasks.\n",
    "            num_tasks: Number of n-shot tasks to group into a single batch\n",
    "        \"\"\"\n",
    "        super(NShotTaskSampler, self).__init__(dataset)\n",
    "        self.episodes_per_epoch = episodes_per_epoch\n",
    "        self.dataset = dataset\n",
    "        if num_tasks < 1:\n",
    "            raise ValueError('num_tasks must be > 1.')\n",
    "\n",
    "        self.num_tasks = num_tasks\n",
    "        # TODO: Raise errors if initialise badly\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.q = q\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.episodes_per_epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.episodes_per_epoch):\n",
    "            batch = []\n",
    "\n",
    "            for task in range(self.num_tasks):\n",
    "                # Get random classes\n",
    "                episode_classes = np.random.choice(\n",
    "                    self.dataset.data_frame['disease_id'].unique(), size=self.k, \n",
    "                                            replace=False)\n",
    "\n",
    "                df = self.dataset.data_frame[\n",
    "                        self.dataset.data_frame['disease_id'].isin(episode_classes)]\n",
    "\n",
    "                support_k = {k: None for k in episode_classes}\n",
    "                for k in episode_classes:\n",
    "                    # Select support examples\n",
    "                    support = df[df['disease_id'] == k].sample(self.n)\n",
    "                    support_k[k] = support\n",
    "\n",
    "                    for i, s in support.iterrows():\n",
    "                        batch.append(s['id'])\n",
    "\n",
    "                for k in episode_classes:\n",
    "                    query = df[(df['disease_id'] == k) & (\n",
    "                        ~df['id'].isin(support_k[k]['id']))].sample(self.q)\n",
    "                    for i, q in query.iterrows():\n",
    "                        batch.append(q['id'])\n",
    "\n",
    "            yield np.stack(batch)\n",
    "            \n",
    "\n",
    "class SpeciesAbundanceDataset(data.Dataset):\n",
    "    \"\"\"Species Abundance dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, abundance_df, species_columns, target_column):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        self.data_frame = abundance_df\n",
    "        self.species_cols = species_columns\n",
    "        self.target_col = target_column\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            target = self.data_frame[self.target_col].iloc[idx].to_numpy(dtype='int')\n",
    "        elif type(idx) != int:\n",
    "            idx = int(idx)\n",
    "            target = self.data_frame[self.target_col].iloc[idx]\n",
    "\n",
    "        species_abundance = self.data_frame[self.species_cols].iloc[idx].to_numpy(\n",
    "                                                                    dtype='float32')\n",
    "        sample = (species_abundance, target)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fae4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, layer_size: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, layer_size)\n",
    "        self.output_fc = nn.Linear(layer_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        h_1 = F.relu(self.input_fc(x))\n",
    "        y_pred = self.output_fc(h_1)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def functional_forward(self, x, weights):\n",
    "        \"\"\"Applies the same forward pass using PyTorch functional \n",
    "        operators using a specified set of weights.\"\"\"\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.linear(x, weights['input_fc.weight'], weights['input_fc.bias'])\n",
    "        x = F.relu(x)\n",
    "        x = F.linear(x, weights['output_fc.weight'], weights['output_fc.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed454b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nshot_task_label(k: int, q: int) -> torch.Tensor:\n",
    "    \"\"\"Creates an n-shot task label.\n",
    "\n",
    "    Label has the structure:\n",
    "        [0]*q + [1]*q + ... + [k-1]*q\n",
    "\n",
    "    # Arguments\n",
    "        k: Number of classes in the n-shot classification task\n",
    "        q: Number of query samples for each class in the n-shot classification task\n",
    "\n",
    "    # Returns\n",
    "        y: Label vector for n-shot task of shape [q * k, ]\n",
    "    \"\"\"\n",
    "    y = torch.arange(0, k, 1 / q).long()\n",
    "\n",
    "    return y\n",
    "\n",
    "def prepare_meta_batch(n, k, q, meta_batch_size):\n",
    "    def prepare_meta_batch_(batch):\n",
    "        x, y = batch\n",
    "        # Reshape to `meta_batch_size` number of tasks. Each task contains\n",
    "        # n*k support samples to train the fast model on and q*k query samples to\n",
    "        # evaluate the fast model on and generate meta-gradients\n",
    "        x = x.reshape(meta_batch_size, n*k + q*k, x.shape[-1])\n",
    "        # Move to device\n",
    "        x = x.double().to(device)\n",
    "        # Create label\n",
    "        y = create_nshot_task_label(k, q).to(device).repeat(meta_batch_size)\n",
    "        return x, y\n",
    "\n",
    "    return prepare_meta_batch_\n",
    "\n",
    "\n",
    "def replace_grad(parameter_gradients, parameter_name):\n",
    "    def replace_grad_(module):\n",
    "        return parameter_gradients[parameter_name]\n",
    "\n",
    "    return replace_grad_\n",
    "\n",
    "def meta_gradient_step(model: nn.Module,\n",
    "                       optimizer: optim.Optimizer,\n",
    "                       loss_fn: Callable,\n",
    "                       x: torch.Tensor,\n",
    "                       y: torch.Tensor,\n",
    "                       n_shot: int,\n",
    "                       k_way: int,\n",
    "                       q_queries: int,\n",
    "                       order: int,\n",
    "                       inner_train_steps: int,\n",
    "                       inner_lr: float,\n",
    "                       train: bool,\n",
    "                       device: Union[str, torch.device]):\n",
    "    \"\"\"\n",
    "    Perform a gradient step on a meta-learner.\n",
    "\n",
    "    # Arguments\n",
    "        model: Base model of the meta-learner being trained\n",
    "        optimizer: Optimizer to calculate gradient step from loss\n",
    "        loss_fn: Loss function to calculate between predictions and outputs\n",
    "        x: Input samples for all few shot tasks\n",
    "        y: Input labels of all few shot tasks\n",
    "        n_shot: Number of examples per class in the support set of each task\n",
    "        k_way: Number of classes in the few shot classification task of each task\n",
    "        q_queries: Number of examples per class in the query set of each task.\n",
    "        The query set is used to calculate\n",
    "            meta-gradients after applying the update to\n",
    "        order: Whether to use 1st order MAML \n",
    "        (update meta-learner weights with gradients of the updated weights on the\n",
    "            query set) or 2nd order MAML (use 2nd order updates by differentiating \n",
    "            through the gradients of the updated weights on the query with respect \n",
    "            to the original weights).\n",
    "        inner_train_steps: Number of gradient steps to fit the fast weights \n",
    "                            during each inner update\n",
    "        inner_lr: Learning rate used to update the fast weights on the inner update\n",
    "        train: Whether to update the meta-learner weights at the end of the episode.\n",
    "        device: Device on which to run computation\n",
    "    \"\"\"\n",
    "    data_shape = x.shape[2:]\n",
    "    create_graph = (True if order == 2 else False) and train\n",
    "\n",
    "    task_gradients = []\n",
    "    task_losses = []\n",
    "    task_predictions = []\n",
    "    for meta_batch in x:\n",
    "        # By construction x is a 5D tensor of shape: \n",
    "        # (meta_batch_size, n*k + q*k, channels, width, height)\n",
    "        # Hence when we iterate over the first  dimension \n",
    "        # we are iterating through the meta batches\n",
    "        x_task_train = meta_batch[:n_shot * k_way]\n",
    "        x_task_val = meta_batch[n_shot * k_way:]\n",
    "\n",
    "        # Create a fast model using the current meta model weights\n",
    "        fast_weights = OrderedDict(model.named_parameters())\n",
    "\n",
    "        # Train the model for `inner_train_steps` iterations\n",
    "        for inner_batch in range(inner_train_steps):\n",
    "            # Perform update of model weights\n",
    "            y = create_nshot_task_label(k_way,n_shot).to(device)\n",
    "            y_pred = model.functional_forward(x_task_train, fast_weights)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            gradients = torch.autograd.grad(loss, fast_weights.values(), \n",
    "                                            create_graph=create_graph)\n",
    "\n",
    "            # Update weights manually\n",
    "            fast_weights = OrderedDict(\n",
    "                (name, param - inner_lr * grad)\n",
    "                for ((name, param), grad) in zip(fast_weights.items(), gradients))\n",
    "\n",
    "        # Do a pass of the model on the validation data from the current task\n",
    "        y = create_nshot_task_label(k_way,q_queries).to(device)\n",
    "        y_pred = model.functional_forward(x_task_val, fast_weights)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Get post-update accuracies\n",
    "        y_prob = y_pred.softmax(dim=1)\n",
    "        task_predictions.append(y_prob)\n",
    "\n",
    "        # Accumulate losses and gradients\n",
    "        task_losses.append(loss)\n",
    "        gradients = torch.autograd.grad(loss, fast_weights.values(), \n",
    "                                        create_graph=create_graph)\n",
    "        named_grads = {name: g for ((name, _), g) in zip(fast_weights.items(), \n",
    "                                                         gradients)}\n",
    "        task_gradients.append(named_grads)\n",
    "\n",
    "    if order == 1:\n",
    "        if train:\n",
    "            sum_task_gradients = {k: torch.stack(\n",
    "                [grad[k] for grad in task_gradients]).mean(dim=0)\n",
    "                                  for k in task_gradients[0].keys()}\n",
    "            hooks = []\n",
    "            for name, param in model.named_parameters():\n",
    "                hooks.append(\n",
    "                    param.register_hook(replace_grad(sum_task_gradients, name))\n",
    "                )\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Dummy pass in order to create `loss` variable\n",
    "            # Replace dummy gradients with mean task gradients using hooks\n",
    "            y_pred = model(torch.zeros((k_way, ) + data_shape).to(device, \n",
    "                                                              dtype=torch.double))\n",
    "            loss = loss_fn(y_pred, create_nshot_task_label(k_way, 1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "\n",
    "        return torch.stack(task_losses).mean(), torch.cat(task_predictions)\n",
    "\n",
    "    elif order == 2:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        meta_batch_loss = torch.stack(task_losses).mean()\n",
    "\n",
    "        if train:\n",
    "            meta_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return meta_batch_loss, torch.cat(task_predictions)\n",
    "    else:\n",
    "        raise ValueError('Order must be either 1 or 2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bea36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def calculate_accuracy(y_pred, y):\n",
    "    return torch.eq(y_pred.argmax(dim=-1), y).sum().item() / y_pred.shape[0]\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, \n",
    "            taskloader: data.DataLoader, \n",
    "            optimizer: optim.Optimizer, \n",
    "            loss_fn: Callable, \n",
    "            prepare_batch: Callable, \n",
    "            **kwargs):\n",
    "    seen = 0\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for batch_index, batch in enumerate(taskloader):\n",
    "        x, y = prepare_batch(batch)\n",
    "\n",
    "        loss, y_pred = meta_gradient_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            **kwargs)\n",
    "\n",
    "        seen += y_pred.shape[0]\n",
    "\n",
    "        total_loss += loss.item() * y_pred.shape[0]\n",
    "        total_acc += calculate_accuracy(y_pred, y) * y_pred.shape[0]\n",
    "\n",
    "    total_loss = total_loss/seen\n",
    "    total_acc = total_acc/seen\n",
    "    return total_loss, total_acc\n",
    "\n",
    "def evaluate(model: nn.Module, \n",
    "            taskloader: data.DataLoader, \n",
    "            optimizer: optim.Optimizer, \n",
    "            loss_fn: Callable, \n",
    "            prepare_batch: Callable, \n",
    "            **kwargs):\n",
    "    seen = 0\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    for batch_index, batch in enumerate(taskloader):\n",
    "        x, y = prepare_batch(batch)\n",
    "\n",
    "        loss, y_pred = meta_gradient_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            **kwargs)\n",
    "\n",
    "        seen += y_pred.shape[0]\n",
    "\n",
    "        total_loss += loss.item()* y_pred.shape[0]\n",
    "        total_acc += calculate_accuracy(y_pred, y) * y_pred.shape[0]\n",
    "\n",
    "    total_loss = total_loss/seen\n",
    "    total_acc = total_acc/seen\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3-shot, 2-way classification ###\n",
    "n_shot = 3\n",
    "k_way = 2\n",
    "q_queries = n_shot\n",
    "meta_batch_size = 2\n",
    "\n",
    "INPUT_DIM = len(select_species)\n",
    "OUTPUT_DIM = k_way\n",
    "LAYER_SIZE = 128\n",
    "inner_lr = 0.01\n",
    "meta_lr = 0.001\n",
    "epoch_len = 800\n",
    "epochs = 30\n",
    "first_order_epochs = 10 #int(0.4*epochs)\n",
    "second_order_epochs = epochs - first_order_epochs\n",
    "eval_batches = 80\n",
    "inner_train_steps = 5\n",
    "inner_val_steps = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "meta_model = MLP(INPUT_DIM, OUTPUT_DIM, LAYER_SIZE).to(device, dtype=torch.double)\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=meta_lr)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "print(f'The model has {count_parameters(meta_model):,} trainable parameters')\n",
    "\n",
    "train_dataset = SpeciesAbundanceDataset(train_df, select_species, 'disease_id')\n",
    "valid_dataset = SpeciesAbundanceDataset(valid_df, select_species, 'disease_id')\n",
    "test_dataset = SpeciesAbundanceDataset(test_df, select_species, 'disease_id')\n",
    "\n",
    "train_taskloader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=NShotTaskSampler(train_dataset, epoch_len, n=n_shot, \n",
    "                                   k=k_way, q=q_queries,\n",
    "                                   num_tasks=meta_batch_size),\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "valid_taskloader = data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_sampler=NShotTaskSampler(valid_dataset, eval_batches, n=n_shot, \n",
    "                                   k=k_way, q=q_queries,\n",
    "                                   num_tasks=meta_batch_size),\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "test_taskloader = data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_sampler=NShotTaskSampler(test_dataset, eval_batches, n=n_shot, \n",
    "                                   k=k_way, q=q_queries,\n",
    "                                   num_tasks=meta_batch_size),\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "\n",
    "train_kwargs = {'n_shot':n_shot, 'k_way': k_way, 'q_queries': q_queries,\n",
    "                'inner_train_steps': inner_train_steps, 'inner_lr': inner_lr,\n",
    "                'order': 1, 'train': True, 'device': device} \n",
    "\n",
    "valid_kwargs = {'n_shot':n_shot, 'k_way': k_way, 'q_queries': q_queries,\n",
    "                'inner_train_steps': inner_val_steps, 'inner_lr': inner_lr,\n",
    "                'order': 1, 'train': True, 'device': device}\n",
    "\n",
    "test_kwargs = {'n_shot':n_shot, 'k_way': k_way, 'q_queries': q_queries,\n",
    "                'inner_train_steps': inner_val_steps, 'inner_lr': inner_lr,\n",
    "                'order': 1, 'train': True, 'device': device}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "for epoch in trange(first_order_epochs):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc = train(meta_model, \n",
    "                                  train_taskloader, \n",
    "                                  meta_optimizer, \n",
    "                                  loss_fn, \n",
    "                                  prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                    meta_batch_size),\n",
    "                                  **train_kwargs\n",
    "                                 )\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(meta_model, \n",
    "                                  valid_taskloader, \n",
    "                                  meta_optimizer, \n",
    "                                  loss_fn, \n",
    "                                  prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                    meta_batch_size),\n",
    "                                  **valid_kwargs\n",
    "                                 )\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(meta_model.state_dict(),'MLP-classifier.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}/{first_order_epochs} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}| Best Val. Loss:'\n",
    "          f'{best_valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "for epoch in trange(second_order_epochs):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_kwargs['order'] = 2\n",
    "    train_loss, train_acc = train(meta_model, \n",
    "                                  train_taskloader, \n",
    "                                  meta_optimizer, \n",
    "                                  loss_fn, \n",
    "                                  prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                    meta_batch_size),\n",
    "                                  **train_kwargs\n",
    "                                 )\n",
    "\n",
    "    valid_kwargs['order'] = 2    \n",
    "    valid_loss, valid_acc = evaluate(meta_model, \n",
    "                                  valid_taskloader, \n",
    "                                  meta_optimizer, \n",
    "                                  loss_fn, \n",
    "                                  prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                    meta_batch_size),\n",
    "                                  **test_kwargs\n",
    "                                 )\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(meta_model.state_dict(),'MLP-classifier.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}/{second_order_epochs} | Epoch Time: '\n",
    "          f'{epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}| Best Val. Loss: '\n",
    "          f'{best_valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_pred, y):\n",
    "    ### calculate confusion matrix, f1_score, roc_auc score\n",
    "    pred_labels = y_pred.argmax(dim=-1).cpu()\n",
    "    true_labels = y.cpu()\n",
    "    cm = metrics.confusion_matrix(true_labels, pred_labels)\n",
    "    f1_score = metrics.f1_score(true_labels, pred_labels)\n",
    "    roc_auc_score = metrics.roc_auc_score(true_labels, pred_labels)\n",
    "    return cm, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_metrics(model: nn.Module, \n",
    "            taskloader: data.DataLoader, \n",
    "            optimizer: optim.Optimizer, \n",
    "            loss_fn: Callable, \n",
    "            prepare_batch: Callable, \n",
    "            **kwargs):\n",
    "    seen = 0\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    for batch_index, batch in enumerate(taskloader):\n",
    "        x, y = prepare_batch(batch)\n",
    "\n",
    "        loss, y_pred = meta_gradient_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            **kwargs)\n",
    "\n",
    "        seen += y_pred.shape[0]\n",
    "\n",
    "        total_loss += loss.item()* y_pred.shape[0]\n",
    "        total_acc += calculate_accuracy(y_pred, y) * y_pred.shape[0]\n",
    "        if batch_index == 0:\n",
    "            total_cm, avg_f1_score, avg_roc_auc_score = calculate_metrics(y_pred,\n",
    "                                                                             y)\n",
    "        else:\n",
    "            cm, f1_score, roc_auc_score = calculate_metrics(y_pred, y)\n",
    "            total_cm += cm\n",
    "            avg_f1_score += f1_score\n",
    "            avg_roc_auc_score += roc_auc_score\n",
    "        \n",
    "    total_loss = total_loss/seen\n",
    "    total_acc = total_acc/seen\n",
    "    avg_f1_score = avg_f1_score/batch_index\n",
    "    avg_roc_auc_score = avg_roc_auc_score/batch_index\n",
    "    return total_loss, total_acc, total_cm, avg_f1_score, avg_roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76041042",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model.load_state_dict(torch.load('MLP-classifier.pt'))\n",
    "\n",
    "test_kwargs['order'] = 2    \n",
    "eval_loss, eval_acc, eval_cm, eval_f1, eval_roc_auc = evaluate_metrics(\n",
    "                                meta_model, \n",
    "                                test_taskloader, \n",
    "                                meta_optimizer, \n",
    "                                loss_fn, \n",
    "                                prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                            meta_batch_size),\n",
    "                                **test_kwargs)\n",
    "\n",
    "print(f'Test Loss: {eval_loss:.3f}| Best Val. Loss: '\n",
    "      f'{best_valid_loss:.3f} |  Test. Acc: {eval_acc*100:.2f}%')\n",
    "print(f'Test F1 score: {eval_f1} | Test ROC AUC score: {eval_roc_auc}')\n",
    "print(f'Confusion matrix:')\n",
    "print(eval_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (py312env)",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
